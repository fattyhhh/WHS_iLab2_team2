{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "\n",
    "def get_all_sublinks(url, max_depth=None, visited=None):\n",
    "    \"\"\"\n",
    "    Recursively retrieve all sublinks from a webpage.\n",
    "\n",
    "    Args:\n",
    "        url (str): The starting URL.\n",
    "        max_depth (int, optional): Maximum depth of recursion. None means no limit.\n",
    "        visited (set, optional): Set of visited URLs to avoid duplicates.\n",
    "\n",
    "    Returns:\n",
    "        set: A set of all sublinks found.\n",
    "    \"\"\"\n",
    "    if visited is None:\n",
    "        visited = set()\n",
    "\n",
    "    if max_depth is not None and max_depth <= 0:\n",
    "        return set()\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            visited.add(url)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            sublinks = set()\n",
    "\n",
    "            for link in soup.find_all('a', href=True):\n",
    "                absolute_link = urljoin(url, link['href'])\n",
    "                if absolute_link not in visited:\n",
    "                    sublinks.add(absolute_link)\n",
    "                    sublinks.update(get_all_sublinks(absolute_link, max_depth - 1, visited))\n",
    "\n",
    "            return sublinks\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "start_url = 'http://nobullconstructions.com.au'  # Replace with your desired starting URL\n",
    "all_sublinks = get_all_sublinks(start_url, max_depth=1)  # Adjust the maximum depth as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.get('http://www.nobullconstructions.com.au').status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
    "requests.packages.urllib3.disable_warnings(InsecureRequestWarning)\n",
    "from urllib.parse import urljoin\n",
    "import xlsxwriter\n",
    "\n",
    "# set headers\n",
    "headers = {\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',\n",
    "    'Accept-Encoding': 'gzip, deflate, br',\n",
    "    'Accept-Language': 'en-US,en;q=0.5',\n",
    "    'Cache-Control': 'max-age=0',\n",
    "    'Connection': 'keep-alive',\n",
    "    'Upgrade-Insecure-Requests': '1',\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:102.0) Gecko/20100101 Firefox/102.0',\n",
    "    'Sec-Fetch-Dest': 'document',\n",
    "    'Sec-Fetch-Mode': 'navigate',\n",
    "    'Sec-Fetch-Site': 'none',\n",
    "    'Sec-Fetch-User': '?1'\n",
    "}\n",
    "all_data = []\n",
    "\n",
    "# import website and company name from excel file\n",
    "urls = pd.read_excel('yellowpages5.xlsx')\n",
    "urls = urls[['name','website']]\n",
    "urls['text'] = ''\n",
    "urls['sublinks'] = ''\n",
    "urls['sublinkstext'] = ''\n",
    "# scrape website text and store in dataframe\n",
    "# something wrong with index 958\n",
    "with requests.Session() as s:\n",
    "            temp = 'grantleigh.com.au/'\n",
    "            r = s.get('https://www.grantleigh.com.au/', verify=False, headers=headers)\n",
    "            soup = BeautifulSoup(r.text, 'html.parser')\n",
    "            text = soup.get_text()\n",
    "            text = text.replace('\\n', '').replace('\\t', '')\n",
    "            sublinks=[]\n",
    "            for link in soup.find_all('a', href=True):\n",
    "                absolute_link = urljoin('https://www.grantleigh.com.au/', link['href'])\n",
    "                if absolute_link not in sublinks:\n",
    "                    if temp in absolute_link:\n",
    "                        sublinks.append(absolute_link)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.grantleigh.com.au/projects', 'https://www.grantleigh.com.au/gallery', 'https://www.grantleigh.com.au/testimonials', 'https://www.grantleigh.com.au/about-grantleigh-homes', 'https://www.grantleigh.com.au/architectural-design-drafting-servi', 'https://www.grantleigh.com.au/construction-rebuild-sydney', 'https://www.grantleigh.com.au/interior-fit-outs', 'https://www.grantleigh.com.au/renovation-extension-builder-sydney']\n"
     ]
    }
   ],
   "source": [
    "print(sublinks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error in sublink\n",
      "mailto:info@hursthomes.com.au\n",
      "error in main link\n",
      "15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\htw10\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error in sublink\n",
      "http://www.jayceesconstructions.com.au/residential-concreting\n",
      "error in sublink\n",
      "http://www.jayceesconstructions.com.au/demolition\n",
      "error in main link\n",
      "55\n",
      "error in main link\n",
      "60\n",
      "error in main link\n",
      "69\n",
      "error in sublink\n",
      "https://malibuhomes.com.au/wp-content/uploads/2022/06/3P5A7411.jpg\n",
      "error in sublink\n",
      "https://malibuhomes.com.au/wp-content/uploads/2022/06/3P5A7469.jpg\n",
      "error in sublink\n",
      "mailto:admin@ptbuilders.com.au\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
    "requests.packages.urllib3.disable_warnings(InsecureRequestWarning)\n",
    "from urllib.parse import urljoin\n",
    "import xlsxwriter\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# set headers\n",
    "headers = {\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',\n",
    "    'Accept-Encoding': 'gzip, deflate, br',\n",
    "    'Accept-Language': 'en-US,en;q=0.5',\n",
    "    'Cache-Control': 'max-age=0',\n",
    "    'Connection': 'keep-alive',\n",
    "    'Upgrade-Insecure-Requests': '1',\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:102.0) Gecko/20100101 Firefox/102.0',\n",
    "    'Sec-Fetch-Dest': 'document',\n",
    "    'Sec-Fetch-Mode': 'navigate',\n",
    "    'Sec-Fetch-Site': 'none',\n",
    "    'Sec-Fetch-User': '?1'\n",
    "}\n",
    "all_data = []\n",
    "\n",
    "# import website and company name from excel file\n",
    "urls = pd.read_excel('yellowpages5.xlsx')\n",
    "urls = urls[['name','website']]\n",
    "urls['text'] = ''\n",
    "urls['sublinks'] = ''\n",
    "urls['sublinkstext'] = ''\n",
    "# scrape website text and store in dataframe\n",
    "# something wrong with index 958\n",
    "with requests.Session() as s:\n",
    "    for i in range(len(urls)):\n",
    "        try:\n",
    "            start_time_1 = time.time()\n",
    "            temp = urls['website'][i].replace('https://www.','')\n",
    "            r = s.get(urls['website'][i], verify=False, headers=headers)\n",
    "            soup = BeautifulSoup(r.text, 'html.parser')\n",
    "            text = soup.get_text()\n",
    "            text = text.replace('\\n', '').replace('\\t', '')\n",
    "            sublinks=[]\n",
    "            for link in soup.find_all('a', href=True):\n",
    "                absolute_link = urljoin(urls['website'][i], link['href'])\n",
    "                if absolute_link not in sublinks:\n",
    "                    if temp in absolute_link:\n",
    "                        sublinks.append(absolute_link)\n",
    "                \n",
    "            urls['text'][i] = text\n",
    "            urls['sublinks'][i] = sublinks\n",
    "            duration_1 = time.time() - start_time_1\n",
    "            for x in urls['sublinks'][i]:\n",
    "                try:\n",
    "                    start_time_2 = time.time()\n",
    "                    t = s.get(x, verify=False, headers=headers)\n",
    "                    soup1 = BeautifulSoup(t.text, 'html.parser')\n",
    "                    text1 = soup1.get_text()\n",
    "                    text1 = text1.replace('\\n', '').replace('\\t', '')\n",
    "                    urls['sublinkstext'][i] = urls['sublinkstext'][i] + text1\n",
    "                    duration_2 = time.time() - start_time_2\n",
    "                    if duration_2 > 300:\n",
    "                        raise TimeoutError\n",
    "                except TimeoutError:\n",
    "                    print('timeout')\n",
    "                    print(x)\n",
    "                    pass\n",
    "                except:\n",
    "                    print('error in sublink')\n",
    "                    print(x)\n",
    "                    pass\n",
    "            \n",
    "            if duration_1 > 600:\n",
    "                raise TimeoutError\n",
    "           \n",
    "        except TimeoutError:\n",
    "            print('timeout')\n",
    "            print(i)\n",
    "            pass\n",
    "        except:\n",
    "            print('error in main link')\n",
    "            print(i)\n",
    "            pass\n",
    "    urls.to_excel('all_text_try.xlsx', engine='xlsxwriter')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>website</th>\n",
       "      <th>text</th>\n",
       "      <th>sublinks</th>\n",
       "      <th>sublinkstext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AC &amp; EA Findlay Constructions</td>\n",
       "      <td>http://www.findlayconstructions.com.au</td>\n",
       "      <td>Findlay Constructions | Expert design and cons...</td>\n",
       "      <td>[http://www.findlayconstructions.com.au, http:...</td>\n",
       "      <td>Findlay Constructions | Expert design and cons...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            name                                 website  \\\n",
       "1  AC & EA Findlay Constructions  http://www.findlayconstructions.com.au   \n",
       "\n",
       "                                                text  \\\n",
       "1  Findlay Constructions | Expert design and cons...   \n",
       "\n",
       "                                            sublinks  \\\n",
       "1  [http://www.findlayconstructions.com.au, http:...   \n",
       "\n",
       "                                        sublinkstext  \n",
       "1  Findlay Constructions | Expert design and cons...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'urls' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\htw10\\OneDrive\\文档\\GitHub\\WHS_iLab2_innovation\\read_son_link_test.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/htw10/OneDrive/%E6%96%87%E6%A1%A3/GitHub/WHS_iLab2_innovation/read_son_link_test.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m urls\u001b[39m.\u001b[39mto_excel(\u001b[39m'\u001b[39m\u001b[39mall_text_6.xlsx\u001b[39m\u001b[39m'\u001b[39m, engine\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mxlsxwriter\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'urls' is not defined"
     ]
    }
   ],
   "source": [
    "urls.to_excel('all_text_6.xlsx', engine='xlsxwriter')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
